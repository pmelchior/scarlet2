{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start Guide\n",
    "\n",
    "This guide shows you how to set up and use *scarlet2* to model a hyperspectral image cube. We assume that you have a full installation of _scarlet2_ including `optax`, `numpyro`, and `h5py`. We'll see below where these packages are needed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Import Packages and setup\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scarlet2 import *"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "We load an example data set (here an image cube from HSC with 5 bands) *and* a detection catalog. If such a catalog is not available, packages like [SEP](http://sep.readthedocs.io/) and [photutils](https://photutils.readthedocs.io/en/stable/) will happily generate one.\n",
    "\n",
    "To make tests like this one convenient, we have a separate HuggingFace data set. To access it, you need to `pip install huggingface_hub`:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "filename = hf_hub_download(repo_id=\"astro-data-lab/scarlet-test-data\", filename=\"hsc_cosmos_35.npz\",\n",
    "                           repo_type=\"dataset\")\n",
    "file = jnp.load(filename)\n",
    "data = jnp.asarray(file[\"images\"])\n",
    "channels = [str(f) for f in file['filters']]\n",
    "centers = jnp.array([(src['y'], src['x']) for src in file[\"catalog\"]])  # Note: y/x convention!\n",
    "weights = jnp.asarray(1 / file[\"variance\"])\n",
    "psf = jnp.asarray(file[\"psfs\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{warning}\n",
    "\n",
    "Coordinates in *scarlet* are given in the C/numpy notation (y,x) as opposed to the more conventional (x,y) ordering.\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Observation and Frame\n",
    "\n",
    "An {py:class}`~scarlet2.Observation` combines several data elements with a {py:class}`~scarlet2.Frame`, similar to header-data arrangement in FITS files. In addition to the actual science image cube, you can and often must provide weights for all elements in the data cube, an image cube of the PSF model (one image for all or one for each channel), an {py:class}`astropy.wcs.WCS` structure to translate from pixel to sky coordinates, and labels for all channels.\n",
    "\n",
    "The {py:class}`~scarlet2.Frame` is a description of the hyperspectral cube of the observations. Think of it as the metadata, what aspects of the sky are described here. At the least, a {py:class}`~scarlet2.Frame` holds the `shape` of the cube, for which we use the convention `(C, H, W)` for the number of elements in 3 dimensions: `C` for the number of bands/channels and `H/W` for the height and width of the images.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "obs = Observation(data,\n",
    "                  weights,\n",
    "                  psf=ArrayPSF(psf),\n",
    "                  channels=channels,\n",
    "                  )\n",
    "model_frame = Frame.from_observations(obs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_scarlet2_ will construct another {py:class}`~scarlet2.Frame` for the _model_ of the sky you seek to fit. It differs from the observation frame because the model needs to be superior in terms of spatial resolution, PSF blurring, spectral coverage, or all of the above, so that the observation can be computed from the model by a degradation operation, the so-called \"forward operator. In _scarlet2_, {py:class}`~scarlet2.renderer.Renderer` implements this operation and contains, e.g., PSF difference kernels and resampling transformations.\n",
    "\n",
    "In this example, we assume that bands and pixel locations are identical between the model and the observation. Because we have ground-based images, the degradation comes from the PSF. With different PSFs in each band, we need to define a reference PSF that is sufficiently narrow. Our default is minimal Gaussian PSF that is barely well-sampled (standard deviation of 0.7 pixels) as our reference kernel:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(model_frame)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If other properties of the model are desired, they can be provided to {py:func}`scarlet2.Frame.from_observations`."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Display Image Cube\n",
    "\n",
    "We can now make use of the plotting function {py:func}`scarlet2.plot.observation` to create a RGB image of our data.\n",
    "We're going to use the {py:class}`~scarlet2.plot.AsinhAutomaticNorm`, which scales the observed data by a {math}`\\mathrm{sinh}^{-1}` function that is automatically tuned to reveal both the noise level and the highlights, to create a color-consistent RGB image."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "norm = plot.AsinhAutomaticNorm(obs)\n",
    "plot.observation(obs, norm=norm, sky_coords=centers, show_psf=True, add_labels=True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use no `wcs` in this {py:class}`~scarlet2.Observation`, all coordinates are already in image pixels, otherwise RA/Dec pairs are expected as `astropy` sky coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Sources\n",
    "\n",
    "You now need to define sources that are going to be fit. The full model, which we will call {py:class}`~scarlet2.Scene`, is a collection of {py:class}`~scarlet2.Source`s. Each source contains at least one {py:class}`~scarlet2.Component`, and each of those is defined by a `center`, `spectrum`, and `morphology`. You can have as many sources in a scene, or as many components in a source, as you want. To represent stars or other point sources, we provide the class {py:class}`~scarlet2.PointSource`, which adopts the PSF of the model frame as the morphological model of the source.\n",
    "\n",
    "To adding sources to a scene is easy. You create the scene (as a python context with the `with` keyword) and then create a source inside of that context. It will automatically be added to the overall scene model. But now we have to decide what the initial values of `spectrum` and `morphology` should be. You can choose them as you see fit, but we also provide convenience methods. For instance {py:func}`scarlet2.init.pixel_spectrum` takes the spectrum of one pixel (typically the center pixel), and {py:func}`scarlet2.init.from_gaussian_moments` measures the spectrum and morphology by measuring the 2nd moments and creating an elliptical Gaussian to match those."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with Scene(model_frame) as scene:\n",
    "    for center in centers:\n",
    "        try:\n",
    "            spectrum, morph = init.from_gaussian_moments(obs, center, min_corr=0.99)\n",
    "        except ValueError:\n",
    "            spectrum = init.pixel_spectrum(obs, center)\n",
    "            morph = init.compact_morphology()\n",
    "\n",
    "        Source(center, spectrum, morph)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know something about the scene, we might want to customize the modeling. Let's assume that we know that object 0 is a star. We could just replace that source with a {py:class}`~scarlet2.PointSource`, but for clarity we rebuild the entire source list, making one change for source 0 and accepting everything else from the default initialization above:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "with Scene(model_frame) as scene:\n",
    "    for i, center in enumerate(centers):\n",
    "        if i == 0:  # we know source 0 is a star\n",
    "            spectrum = init.pixel_spectrum(obs, center, correct_psf=True)\n",
    "            PointSource(center, spectrum)\n",
    "        else:\n",
    "            try:\n",
    "                spectrum, morph = init.from_gaussian_moments(obs, center, min_corr=0.99)\n",
    "            except ValueError:\n",
    "                spectrum = init.pixel_spectrum(obs, center)\n",
    "                morph = init.compact_morphology()\n",
    "\n",
    "            Source(center, spectrum, morph)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "We can now check what we have in `scene`:",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": "print(scene)",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scene` contains the `frame` and the list `sources`. The first source is indeed a {py:class}`~scarlet2.PointSource`, which has a `morphology` model, namely the same {py:class}`~scarlet2.GaussianMorphology` as the PSF of the model frame.\n",
    " All others are standard {py:class}`~scarlet2.Source`s. The data portions in these models are listed as, e.g., `f32[11,11]`, which denotes an image array of 11x11 pixels. The size of these boxes was determined by {py:func}`scarlet2.init.from_gaussian_moments` to contain most of the flux (or the largest box that seems to be occupied by only one source)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model\n",
    "\n",
    "The {py:class}`~scarlet2.Scene` class holds the list of sources and creates a model of the sky that (like any {py:class}`~scarlet2.Module`) can be evaluated, which allows us to determine the log-likelihood of the data given the model:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "scene_array = scene()  # evaluate the model\n",
    "print(\"Initial likelihood:\", obs.log_likelihood(scene_array))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "You can now write your own fitter or sampler, but we provide the machinery for that as well. We're using `optax` for the optimization. First we need to define which {py:class}`~scarlet2.Parameters` of the model should be optimized. For each {py:class}`~scarlet2.Parameter`, that includes a stepsize, and, optionally, differentiable constraints from `numpyro`. Below is our recommended configuration:",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "from numpyro.distributions import constraints\n",
    "\n",
    "# best step size for parameters with a relative \"uncertainty\":\n",
    "# big/bright sources adjust more quickly\n",
    "spec_step = partial(relative_step, factor=0.05)\n",
    "morph_step = partial(relative_step, factor=1e-3)\n",
    "\n",
    "parameters = scene.make_parameters()\n",
    "for i in range(len(scene.sources)):\n",
    "    parameters += Parameter(scene.sources[i].spectrum,\n",
    "                            name=f\"spectrum:{i}\",\n",
    "                            constraint=constraints.positive,\n",
    "                            stepsize=spec_step)\n",
    "    if i == 0:\n",
    "        parameters += Parameter(scene.sources[i].center,\n",
    "                                name=f\"center:{i}\",\n",
    "                                stepsize=0.1)\n",
    "    else:\n",
    "        parameters += Parameter(scene.sources[i].morphology,\n",
    "                                name=f\"morph:{i}\",\n",
    "                                constraint=constraints.unit_interval,\n",
    "                                stepsize=morph_step)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "A model parameter that is not listed in `parameters` will not be updated. Now we can run the fitting method, and update every one of the `parameters`. We give ourselves a head-start by first running a linear solver for the spectra given the shapes of every source:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "maxiter = 1000\n",
    "scene.set_spectra_to_match(obs, parameters)\n",
    "scene_ = scene.fit(obs, parameters, max_iter=maxiter, e_rel=1e-3, progress_bar=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Interact with Results\n",
    "\n",
    "The updated source models are now available in `scene_`. We can evaluate the entire scene and compute the now improved likelihood:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scene_array = scene_()\n",
    "print(\"Optimized likelihood:\", obs.log_likelihood(scene_array))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Display Full Scene"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plot.scene(scene_,\n",
    "           obs,\n",
    "           norm=norm,\n",
    "           show_model=True,\n",
    "           show_rendered=True,\n",
    "           show_observed=True,\n",
    "           show_residual=True,\n",
    "           add_boxes=True,\n",
    "           )\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The fit is overall quite good, with low residuals, but it's not perfect. Let's investigate..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Access All Sources\n",
    "\n",
    "Individual sources can be accessed and evaluated as well, but we have to decide in which frame we want to see them. There are three options:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get models for source 1\n",
    "source = scene_.sources[1]\n",
    "# in its own source box (indicated by the white boxes in the figure above)\n",
    "source_array = source()\n",
    "# inserted in model frame\n",
    "source_in_scene_array = scene_.evaluate_source(source)\n",
    "# as it appears in observed frame\n",
    "source_as_seen = obs.render(source_in_scene_array)\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# for imshow: don't interpolate the pixels, put origin to bottom\n",
    "matplotlib.rc('image', interpolation='none', origin='lower')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
    "axes[0].imshow(plot.img_to_rgb(source_array, norm=norm))\n",
    "axes[1].imshow(plot.img_to_rgb(source_in_scene_array, norm=norm))\n",
    "axes[2].imshow(plot.img_to_rgb(source_as_seen, norm=norm))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also provide a function to create all of these source images:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plot.sources(scene_,\n",
    "             norm=norm,\n",
    "             observation=obs,\n",
    "             show_model=True,\n",
    "             show_rendered=True,\n",
    "             show_observed=True,\n",
    "             show_spectrum=True,\n",
    "             add_markers=False,\n",
    "             add_boxes=True,\n",
    "             )\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each source \"lives\" in a smaller box and is then placed into the larger scene. The model of object 0 assumes the simple Gaussian shape of the model PSF, which is the internal representation of a point source.\n",
    "\n",
    "It's noticeable that all other sources, however, show pixel patterns that don't seem right. Some of them follow other sources (especially source 1) or noise (sources 4-6). What we see here is the limitation of non-parametric models: they can fit anything, but the likelihood may not have enough information to constrain all of these degrees of freedom. More/stronger constraints or priors can help, and we will describe their use in [a separate tutorial](howto/priors).\n",
    "\n",
    "### Source Measurements\n",
    "\n",
    "As shown above, source models are generated in the model frame (dah...), from which any measurement can directly be made without having to deal with noise, PSF convolution, overlapping sources, etc.\n",
    "\n",
    "For instance, the color information in these plots stems from the spectrum, i.e. the per-band amplitude, which are computed from the hyperspectral model by integrating over the morphology. The source spectra plots in the right panels above have done exactly that. The convention of these fluxes is given by the units and ordering of the original data cube."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"----------------- {}\".format(channels))\n",
    "for k, src in enumerate(scene_.sources):\n",
    "    print(\"Source {}, Fluxes: {}\".format(k, measure.flux(src)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other measurements (e.g. `centroid`) or {py:class}`~scarlet2.measure.Moments` are also implemented:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "g = measure.Moments(scene_.sources[1])\n",
    "print(\"Source 1 ellipticity:\", g.ellipticity)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of these measurements are ordered by channel, so the ellipticity above is listed as `[[e1_g, e1_r, e1_i, e1_z, e1_y],[e2_g, e2_r, e2_i, e2_z, e2_y]]`. That they are all the same should not come as a surprise: For single-component models, there is no variation of the morphology across the channels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Re-Use Model\n",
    "\n",
    "To preserve the model for posterity, individual sources (or their sub-models) or entire scenes can be serialized into a HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import scarlet2.io\n",
    "\n",
    "id = 35\n",
    "filename = \"hsc_cosmos.h5\"\n",
    "scarlet2.io.model_to_h5(scene_, filename, id=id, overwrite=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stored model be loaded in the same way. Every source can be utilized as before."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scene__ = scarlet2.io.model_from_h5(filename, id=id)\n",
    "plot.scene(scene__, observation=obs, norm=norm, add_boxes=True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add two more sources to account for the largest residuals we have seen above. As we don't know their location accurately, we allow the fitter to shift/recenter the sources."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# enter the context of the scene to create new elements\n",
    "with scene__:\n",
    "    # add two marginally detected sources at their approximate locations\n",
    "    yx = [(14., 44.), (42., 9.)]\n",
    "    for center in yx:\n",
    "        center = jnp.array(center)\n",
    "        spectrum = init.pixel_spectrum(obs, center)\n",
    "        morph = init.compact_morphology()\n",
    "        Source(center, spectrum, morph)\n",
    "\n",
    "# need to remake the parameter structure because we have more free parameters now\n",
    "parameters__ = scene__.make_parameters()\n",
    "for i in range(len(scene__.sources)):\n",
    "    parameters__ += Parameter(scene__.sources[i].spectrum,\n",
    "                              name=f\"spectrum:{i}\",\n",
    "                              constraint=constraints.positive,\n",
    "                              stepsize=spec_step)\n",
    "    if i == 0:\n",
    "        parameters__ += Parameter(scene__.sources[i].center,\n",
    "                                  name=f\"center:{i}\",\n",
    "                                  stepsize=0.1)\n",
    "    else:\n",
    "        parameters__ += Parameter(scene__.sources[i].morphology,\n",
    "                                  name=f\"morph:{i}\",\n",
    "                                  constraint=constraints.unit_interval,\n",
    "                                  stepsize=0.1)\n",
    "\n",
    "scene___ = scene__.fit(obs, parameters__, max_iter=maxiter, e_rel=1e-4, progress_bar=False)\n",
    "print(\"Re-optimized likelihood:\", obs.log_likelihood(scene___()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the log-likelihood is slightly better than before. Let's have a look at the new model:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot.scene(scene___,\n",
    "           obs,\n",
    "           norm=norm,\n",
    "           show_model=True,\n",
    "           show_rendered=True,\n",
    "           show_observed=True,\n",
    "           show_residual=True,\n",
    "           add_boxes=True\n",
    "           )\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the two new sources 7 and 8, and that most of the other features are very similar to before. As expected, the residuals have visibly improved where we added the source and are now dominated by a red-blue pattern in the center of source 1, which we should probably fit with 2 components."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
